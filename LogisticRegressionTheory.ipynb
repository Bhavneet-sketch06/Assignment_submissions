{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d29b114",
   "metadata": {},
   "source": [
    "Q1.What is Logistic Regression, and how does it differ from Linear Regression.\n",
    "Ans.1. Logistic Regression\n",
    "Purpose: Used for classification problems (mainly binary classification).\n",
    "Output: Predicts probabilities (values between 0 and 1).\n",
    "Equation: Uses the sigmoid (logistic) function to transform the output of a linear equation into a probability:\n",
    "P(Y=1)=  1 / 1+e^−(b0+b1X1 +b2X2+...+bnXn)\n",
    "\n",
    "If P(Y=1) > 0.5, classify as 1\n",
    "If P(Y=1) ≤ 0.5, classify as 0\n",
    "Since the output is a probability, logistic regression is used in problems like spam detection, medical diagnosis, and credit risk analysis.\n",
    "\n",
    "2. Linear Regression\n",
    "Purpose: Used for regression problems (predicting continuous values).\n",
    "Output: Predicts a numerical value.\n",
    "Equation: A simple linear relationship between input variables X and output Y:\n",
    "Y = b0+b1X1+b2X2+...+bnXn\n",
    "It is useful in predicting house prices, sales, or temperature trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48743e88",
   "metadata": {},
   "source": [
    "Q2.What is the mathematical equation of Logistic Regression.\n",
    "Ans.The mathematical equation of Logistic Regression is based on the sigmoid (logistic) function, which transforms a linear equation into a probability.\n",
    "\n",
    "Logistic Regression Model\n",
    "For a given set of input features X1,X2,...,Xn , the logistic regression model calculates the probability of the dependent variable Y belonging to class 1 (e.g., Yes, True, Success):\n",
    "P(Y=1)=  1 / 1+e^−(b0+b1X1 +b2X2+...+bnXn)\n",
    "where:\n",
    "P(Y=1) is the probability that \n",
    "Y belongs to class 1.\n",
    "b0 (intercept) and b1,b2,...,bn (coefficients) are the parameters of the model.\n",
    "X1,X2,...,Xn are the independent variables (input features).\n",
    "e is the base of the natural logarithm (~2.718).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b9bafd",
   "metadata": {},
   "source": [
    "Q3. Why do we use the Sigmoid function in Logistic Regression?\n",
    "Ans.The sigmoid function (also called the logistic function) is used in Logistic Regression to map any real-valued number into a probability between 0 and 1. Since classification problems require outputs in this range, the sigmoid function is ideal for this purpose.\n",
    "Reasons for Using the Sigmoid Function\n",
    "(i) Converts Output into a Probability\n",
    "Linear functions can output any real number (−∞,∞), which isn't suitable for classification.\n",
    "The sigmoid function compresses this range into (0,1), representing a probability.\n",
    "(ii) Enables Classification\n",
    "If P(Y=1)>0.5, classify as 1 (positive class).\n",
    "If P(Y=1)≤0.5, classify as 0 (negative class).\n",
    "(iii) Provides a Smooth, Differentiable Function\n",
    "The sigmoid function is smooth and continuous, making it suitable for optimization using Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb56bc",
   "metadata": {},
   "source": [
    "Q4.What is the cost function of Logistic Regression?\n",
    "Ans.In Logistic Regression, we use a different cost function than in Linear Regression because the squared error cost function used in Linear Regression is not convex for Logistic Regression, making it difficult to optimize. Instead, we use the Log-Loss (Logistic Loss) function, also known as the Binary Cross-Entropy Loss.\n",
    "1. Cost Function Definition\n",
    "For a single training example (X,Y), the cost function is:\n",
    "J(θ)=−[Ylog(hθ(X))+(1−Y)log(1−hθ(X))]\n",
    "where:\n",
    "(X) is the predicted probability:\n",
    "hθ(X)= 1 / 1+e−θTX\n",
    "\n",
    "Y is the actual class label (0 or 1).\n",
    "θ represents the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0c4a0",
   "metadata": {},
   "source": [
    "Q5.What is Regularization in Logistic Regression? Why is it needed.\n",
    "Ans.Regularization is a technique used in Logistic Regression (and other machine learning models) to prevent overfitting by adding a penalty to the model’s cost function. This helps keep the model simpler and more generalizable to new data.\n",
    "Why is Regularization Needed?\n",
    "In Logistic Regression, the model learns weights (θ) that best separate the data. However, if the model is too complex (too many features or high variance), it may:\n",
    "\n",
    "Overfit the training data (performs well on training but poorly on test data).\n",
    "Have large parameter values (θ) leading to excessive sensitivity to small variations in input data.\n",
    "Regularization controls the magnitude of these parameters to avoid overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448838e",
   "metadata": {},
   "source": [
    "Q6.Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "Ans.\n",
    "1. Lasso Regression (L1 Regularization)\n",
    "Lasso stands for Least Absolute Shrinkage and Selection Operator.\n",
    "\n",
    "Mathematical Equation:\n",
    "The cost function for Lasso adds the absolute values of the coefficients as a penalty:\n",
    "J(θ)=Loss function+λ j=1∑n∣θj∣\n",
    "Key Features:\n",
    " Feature Selection – Shrinks some coefficients to exactly zero, effectively removing irrelevant features.\n",
    " Helps in creating sparse models by selecting only the most important variables.\n",
    " Useful when dealing with high-dimensional data (many features).\n",
    "\n",
    "2. Ridge Regression (L2 Regularization)\n",
    "Ridge regression adds a squared penalty to the cost function, preventing large coefficients but keeping all features.\n",
    "\n",
    "Mathematical Equation:\n",
    "J(θ)= Loss function + λ j=1 ∑ n θj2\n",
    "Key Features:\n",
    "No feature elimination – Unlike Lasso, Ridge regression does not shrink coefficients to zero, but it makes them smaller.\n",
    "Helps when all features contribute to the prediction.\n",
    "Works well when features are correlated.\n",
    "\n",
    "3. Elastic Net Regression (Combination of L1 & L2)\n",
    "Elastic Net is a combination of both Lasso (L1) and Ridge (L2), balancing feature selection and coefficient shrinkage.\n",
    "\n",
    "Mathematical Equation:\n",
    "J(θ)=Loss function+λ1 j=1∑n∣θj∣ + λ2 j=1 ∑ n θj2\n",
    "Key Features:\n",
    "Combines benefits of Lasso & Ridge – Performs feature selection (L1) and prevents overfitting (L2).\n",
    "Works well in high-dimensional datasets with correlated features.\n",
    "More flexible as it allows tuning of both penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f0a3a",
   "metadata": {},
   "source": [
    "Q7.When should we use Elastic Net instead of Lasso or Ridge?\n",
    "Ans.Use Elastic Net When:\n",
    "1. Features are Highly Correlated (Multicollinearity)\n",
    " Lasso struggles with correlated variables and may arbitrarily drop one while keeping another.\n",
    " Ridge keeps all variables but does not perform feature selection.\n",
    " Elastic Net works best when features are correlated by balancing L1 and L2 penalties.\n",
    "2. There Are Many Features (High-Dimensional Data)\n",
    " Lasso can eliminate too many features when there are many irrelevant variables.\n",
    " Ridge retains all features, making interpretation harder.\n",
    " Elastic Net handles high-dimensional data better by selecting relevant features while keeping important correlated ones.\n",
    "3. You Need Both Feature Selection & Shrinkage\n",
    " Lasso might remove too many features, leading to underfitting.\n",
    " Ridge does not remove features but can lead to overfitting in large datasets.\n",
    " Elastic Net provides a balance by selecting relevant features while preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef51190",
   "metadata": {},
   "source": [
    "Q8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "Ans.Impact of the Regularization Parameter (λ) in Logistic Regression\n",
    "The regularization parameter (λ) controls the strength of the penalty term added to the cost function in Logistic Regression. It affects how the model learns from the data by controlling the trade-off between model complexity and overfitting.\n",
    "Effects of λ on Model Performance\n",
    "-When λ is Too Small (Near 0)\n",
    "Minimal regularization → The model behaves like standard Logistic Regression.\n",
    "Large coefficients (θ) → Model is complex, capturing noise.\n",
    "Overfitting risk → Model fits training data well but performs poorly on unseen data.\n",
    "\n",
    "-When λ is Too Large\n",
    "Strong regularization → Coefficients shrink significantly.\n",
    "Model is too simple → Can cause underfitting.\n",
    "Important features may be ignored → Reduces model performance.\n",
    "\n",
    "-Optimal λ (Balanced Regularization)\n",
    "Avoids overfitting while keeping useful features.\n",
    "Helps generalization, leading to better performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8d343",
   "metadata": {},
   "source": [
    "Q9.What are the key assumptions of Logistic Regression?\n",
    "Ans.Key Assumptions of Logistic Regression\n",
    "Logistic Regression is a widely used classification algorithm, but like any statistical model, it relies on certain assumptions for optimal performance.\n",
    "1. The Dependent Variable is Binary (for Binary Logistic Regression)\n",
    " Logistic Regression assumes that the target variable (Y) is binary (0 or 1).\n",
    " If the response variable has multiple categories, Multinomial Logistic Regression or Ordinal Logistic Regression should be used instead.\n",
    "2. No High Multicollinearity Among Independent Variables\n",
    "Logistic Regression assumes low or no multicollinearity (high correlation among independent variables).\n",
    "Why?\n",
    "High correlation between features makes it difficult to determine their individual effects on the target variable.\n",
    "Solution:\n",
    "Use Variance Inflation Factor (VIF) to detect multicollinearity.\n",
    "Apply Principal Component Analysis (PCA) or remove correlated features.\n",
    "3. The Dataset is Sufficiently Large\n",
    " Logistic Regression performs best when the dataset has a sufficient number of observations per predictor variable.\n",
    " Rule of thumb: At least 10-20 observations per feature for stable coefficient estimates.\n",
    "\n",
    " Small dataset issues:\n",
    "Model may become unstable or fail to converge.\n",
    "Consider Bayesian Logistic Regression or Penalized Regression (Ridge/Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6e854",
   "metadata": {},
   "source": [
    "Q10.What are some alternatives to Logistic Regression for classification tasks.\n",
    "Ans.Alternatives to Logistic Regression for Classification Tasks\n",
    "While Logistic Regression is a simple and interpretable classification algorithm, it may not always be the best choice—especially when dealing with non-linearity, high-dimensional data, or complex relationships. Here are some powerful alternatives:\n",
    "\n",
    "1. Decision Trees\n",
    "Best for: Interpretable models, capturing non-linear relationships.\n",
    " Splits data into branches based on feature values.\n",
    " Handles non-linearity and interactions between features.\n",
    " Works well with both numerical and categorical data.\n",
    " Can overfit if not pruned (Regularization helps).\n",
    "2. Random Forest \n",
    " Best for: High accuracy, robustness, handling missing values.\n",
    " Ensemble method – Combines multiple Decision Trees for better predictions.\n",
    " Reduces overfitting (unlike a single Decision Tree).\n",
    " Works well on large datasets and high-dimensional data.\n",
    " Handles imbalanced datasets with class weighting.\n",
    "3. Naïve Bayes \n",
    " Best for: Text classification, spam detection, sentiment analysis.\n",
    " Assumes independent features (though not always realistic).\n",
    " Fast, scalable for large datasets.\n",
    " Works well when data is categorical (e.g., words in text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cd24c0",
   "metadata": {},
   "source": [
    "11.What are Classification Evaluation Metrics.\n",
    "\n",
    "Classification evaluation metrics are used to assess the performance of a classification model. Here are some of the most common ones:\n",
    "\n",
    "Accuracy Measures the percentage of correctly classified instances. Formula: 𝐴 𝑐 𝑐 𝑢 𝑟 𝑎 𝑐 𝑦 = 𝑇 𝑃\n",
    "+ 𝑇 𝑁 𝑇 𝑃 + 𝑇 𝑁 + 𝐹 𝑃 + 𝐹 𝑁 Accuracy= TP+TN+FP+FN TP+TN​\n",
    "\n",
    "Best when classes are balanced.\n",
    "\n",
    "Precision (Positive Predictive Value) Measures how many of the predicted positive instances are actually positive. Formula: 𝑃 𝑟 𝑒 𝑐 𝑖 𝑠 𝑖 𝑜 𝑛 = 𝑇 𝑃 𝑇 𝑃\n",
    "+ 𝐹 𝑃 Precision= TP+FP TP​\n",
    "\n",
    "Useful when false positives are costly.\n",
    "\n",
    "Recall (Sensitivity, True Positive Rate) Measures how many actual positive instances are correctly identified. Formula: 𝑅 𝑒 𝑐 𝑎 𝑙 𝑙 = 𝑇 𝑃 𝑇 𝑃\n",
    "+ 𝐹 𝑁 Recall= TP+FN TP​\n",
    "\n",
    "Important when false negatives are costly.\n",
    "\n",
    "F1-Score Harmonic mean of precision and recall. Formula: 𝐹 1 = 2 × 𝑃 𝑟 𝑒 𝑐 𝑖 𝑠 𝑖 𝑜 𝑛 × 𝑅 𝑒 𝑐 𝑎 𝑙 𝑙 𝑃 𝑟 𝑒 𝑐 𝑖 𝑠 𝑖 𝑜 𝑛\n",
    "+ 𝑅 𝑒 𝑐 𝑎 𝑙 𝑙 F1=2× Precision+Recall Precision×Recall​\n",
    "\n",
    "Useful when the dataset is imbalanced.\n",
    "\n",
    "Confusion Matrix A table that shows the actual vs. predicted classifications. It includes: True Positives (TP) – Correctly predicted positive cases. True Negatives (TN) – Correctly predicted negative cases. False Positives (FP) – Incorrectly predicted positive cases. False Negatives (FN) – Incorrectly predicted negative cases.\n",
    "ROC Curve (Receiver Operating Characteristic) Plots the True Positive Rate (TPR) vs. False Positive Rate (FPR) at various thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44e027",
   "metadata": {},
   "source": [
    "12.How does class imbalance affect Logistic Regression.\n",
    "\n",
    "Class imbalance significantly affects Logistic Regression and other classification models in several ways:\n",
    "\n",
    "Biased Model Towards the Majority Class Logistic Regression minimizes the log-loss function, which can be heavily influenced by the majority class. The model may predict the majority class most of the time, leading to high accuracy but poor recall for the minority class.\n",
    "Poor Recall for the Minority Class Since the model is biased towards the majority class, it often fails to correctly classify instances of the minority class. High false negatives can be problematic in applications like fraud detection or medical diagnosis.\n",
    "Skewed Decision Boundary The model learns a decision boundary that favors the majority class. This results in fewer correctly classified minority class samples.\n",
    "Misleading Performance Metrics Accuracy can be misleading in imbalanced datasets. For example, in a dataset with 95% majority and 5% minority class, predicting all samples as the majority class gives 95% accuracy but is useless for detecting the minority class. Metrics like Precision, Recall, F1-score, and AUC-ROC provide a better evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961de9c",
   "metadata": {},
   "source": [
    "13.What is Hyperparameter Tuning in Logistic Regression\n",
    "\n",
    "Key Hyperparameters in Logistic Regression Penalty (penalty)\n",
    "\n",
    "Regularization technique to prevent overfitting. Options: 'l1' (Lasso - feature selection) 'l2' (Ridge - weight shrinkage) 'elasticnet' (Combination of L1 & L2) 'none' (No regularization) Regularization Strength (C)\n",
    "\n",
    "Controls the inverse of regularization strength (smaller C means stronger regularization). Higher values allow more flexibility, while lower values restrict the model. Solver (solver)\n",
    "\n",
    "Determines the optimization algorithm. Options: 'lbfgs' (default, good for small to medium datasets) 'liblinear' (good for small datasets, supports L1 & L2) 'saga' (efficient for large datasets, supports all penalties) Maximum Iterations (max_iter)\n",
    "\n",
    "Number of iterations for convergence. Increase if the model fails to converge. Class Weight (class_weight)\n",
    "\n",
    "Adjusts weight for imbalanced datasets. Options: 'balanced' (automatically adjusts based on class frequencies) {0: weight, 1: weight} (custom weight assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db907b1a",
   "metadata": {},
   "source": [
    "14.What are different solvers in Logistic Regression? Which one should be used\n",
    "\n",
    "Solvers in Logistic Regression Solvers are optimization algorithms used to minimize the loss function in Logistic Regression. Different solvers are optimized for different types of datasets and regularization techniques.\n",
    "\n",
    "lbfgs (Limited-memory BFGS) Type: Quasi-Newton method Supports: L2 regularization Best for: Small to medium datasets Pros: Fast and memory-efficient Cons: Does not support L1 regularization Recommended Default Solver (used when no solver is specified)\n",
    "\n",
    "liblinear (Library for Large Linear Classification) Type: Coordinate Descent Supports: L1 & L2 regularization Best for: Small datasets & sparse data Pros: Supports both L1 & L2 penalties Cons: Slow for large datasets Use when L1 regularization is needed\n",
    "\n",
    "saga (Stochastic Average Gradient) Type: Stochastic Gradient Descent (SGD) variant Supports: L1, L2 & Elastic-Net regularization Best for: Large datasets, sparse data Pros: Handles large datasets efficiently Cons: Requires more tuning (e.g., learning rate) Use for large-scale datasets or when using Elastic-Net\n",
    "\n",
    "newton-cg (Newton Conjugate Gradient) Type: Second-order optimization method Supports: L2 regularization Best for: Large datasets with many features Pros: Converges faster than lbfgs for high-dimensional data Cons: More computationally expensive Use when dealing with many features\n",
    "\n",
    "sag (Stochastic Average Gradient) Type: First-order stochastic method Supports: L2 regularization Best for: Large datasets Pros: Works well for large datasets with high feature counts Cons: Can struggle with small datasets Use for large datasets with L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51cefcf",
   "metadata": {},
   "source": [
    "15.How is Logistic Regression extended for multiclass classification.\n",
    "\n",
    "One-vs-Rest (OvR) / One-vs-All (OvA) The most common method. The model trains one classifier per class, treating it as the \"positive\" class while all other classes are \"negative.\" During prediction, the class with the highest probability is chosen. How It Works For a dataset with 3 classes (A, B, C): Model 1: Class A vs. (B & C) Model 2: Class B vs. (A & C) Model 3: Class C vs. (A & B) The model picks the class with the highest predicted probability.\n",
    "\n",
    "One-vs-One (OvO) Instead of training one model per class, it trains a model for every pair of classes. For N classes, it trains N*(N-1)/2 binary classifiers. During prediction, it uses a voting system: each classifier votes for a class, and the class with the most votes wins.\n",
    "\n",
    "Softmax Regression (Multinomial Logistic Regression) Instead of breaking it into multiple binary problems, it directly predicts one of the classes.\n",
    "\n",
    "Uses the Softmax function to compute probabilities for all classes:\n",
    "\n",
    "𝑃 ( 𝑦 = 𝑘 ∣ 𝑥 ) = 𝑒 𝜃 𝑘 𝑇 𝑥 ∑ 𝑗 = 1 𝐾 𝑒 𝜃 𝑗 𝑇 𝑥 P(y=k∣x)= ∑ j=1 K​e θ j T​x\n",
    "\n",
    "e θ k T​x\n",
    "\n",
    "​\n",
    "\n",
    "Optimized using cross-entropy loss instead of binary log-loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189305f",
   "metadata": {},
   "source": [
    "16.What are the advantages and disadvantages of Logistic Regression\n",
    "\n",
    "Advantages of Logistic Regression\n",
    "\n",
    "Simple and Interpretable Easy to implement and interpret. The coefficients provide insights into feature importance.\n",
    "Computationally Efficient Faster to train compared to complex models like neural networks. Works well with small to medium-sized datasets.\n",
    "Works Well with Linearly Separable Data Performs well when classes can be separated with a straight decision boundary.\n",
    "Probabilistic Interpretation Outputs probabilities, allowing for better decision-making. Useful in risk assessment (e.g., fraud detection, medical diagnosis).\n",
    "Handles Binary and Multiclass Classification Can be extended to multiclass problems using One-vs-Rest (OvR) or Softmax (Multinomial Logistic Regression).\n",
    "Regularization Support Supports L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting.\n",
    "Disadvantages of Logistic Regression\n",
    "\n",
    "Assumes Linearity Cannot model complex, non-linear relationships between features and target variables. May perform poorly when decision boundaries are non-linear.\n",
    "Sensitive to Outliers Outliers can significantly impact the coefficients and decision boundary.\n",
    "Not Ideal for High-Dimensional Data Struggles when there are too many features (high-dimensional space). Can lead to overfitting if not regularized properly.\n",
    "Requires Feature Engineering Works best when important features are manually selected or transformed. May need polynomial features or interaction terms to improve performance.\n",
    "Imbalanced Data Problem Biased towards the majority class in highly imbalanced datasets. Needs techniques like class weighting, SMOTE (oversampling), or threshold tuning.\n",
    "Can Struggle with Large Datasets Though efficient, logistic regression may not scale well to massive datasets with millions of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b26b4d",
   "metadata": {},
   "source": [
    "17.What are some use cases of Logistic Regression.\n",
    "\n",
    "Medical Diagnosis Example: Predicting whether a patient has a disease (Yes/No). Use Case: Cancer detection (e.g., breast cancer classification). Diabetes prediction. Heart disease risk assessment.\n",
    "Fraud Detection Example: Detecting fraudulent transactions in banking and e-commerce. Use Case: Credit card fraud detection. Identifying suspicious login attempts. Insurance fraud detection.\n",
    "Customer Churn Prediction Example: Predicting whether a customer will stop using a service (Churn/No Churn). Use Case: Telecom companies predicting customer retention. Subscription-based businesses (Netflix, Spotify) reducing churn. E-commerce customer retention strategies.\n",
    "Marketing and Advertising Example: Predicting if a user will click on an ad (Click/No Click). Use Case: Email marketing campaigns (predicting open rates). Personalized advertisements (targeted offers). Customer segmentation (classifying high-value customers).\n",
    "Credit Scoring and Loan Approval Example: Predicting if a person will default on a loan (Default/No Default). Use Case: Loan approval processes in banks. Assessing creditworthiness using historical data. Mortgage risk assessment.\n",
    "Human Resources (HR Analytics) Example: Predicting if an employee will leave the company (Leave/Stay). Use Case: Employee attrition prediction. Identifying factors influencing job satisfaction. Predicting successful hires.\n",
    "Spam Detection Example: Classifying emails as spam or not spam. Use Case: Email filtering (Gmail, Outlook). SMS fraud detection. Chatbot moderation (detecting inappropriate messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46292874",
   "metadata": {},
   "source": [
    "18.What is the difference between Softmax Regression and Logistic Regression\n",
    "\n",
    "Number of classes\n",
    "\n",
    "Logistic Regression is used for binary classification, meaning it predicts one of two possible outcomes (e.g., spam vs. not spam). Softmax Regression, also known as Multinomial Logistic Regression, is used for multiclass classification, meaning it predicts one of three or more categories (e.g., classifying images of dogs, cats, and birds).\n",
    "\n",
    "Mathematical Function Used\n",
    "\n",
    "Logistic Regression applies the sigmoid function, which maps values to a range between 0 and 1. This is useful when there are only two classes, as it predicts the probability of one class. Softmax Regression applies the softmax function, which calculates the probability of each class and ensures the probabilities sum to 1. This allows the model to predict multiple classes at once.\n",
    "\n",
    "Decision Making\n",
    "\n",
    "In Logistic Regression, if the predicted probability is greater than 0.5, the model classifies the input into class 1; otherwise, it classifies it as class 0. In Softmax Regression, the model assigns a probability to each possible class, and the class with the highest probability is chosen. Loss Function\n",
    "\n",
    "Logistic Regression uses binary cross-entropy (log loss) to measure error because it deals with two classes. Softmax Regression uses categorical cross-entropy, which is a generalization of log loss for multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8943c",
   "metadata": {},
   "source": [
    "19.How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification\n",
    "\n",
    "One-vs-Rest (OvR) Approach\n",
    "How It Works\n",
    "\n",
    "Trains one binary classifier per class. Each classifier treats its class as positive (1) and all other classes as negative (0). During prediction, the class with the highest probability wins.\n",
    "\n",
    "When to Choose OvR\n",
    "\n",
    "Small to medium-sized datasets – Since it trains multiple binary models, it performs well when data is limited.\n",
    "\n",
    "Sparse or imbalanced datasets – If some classes have very few samples, OvR works better than Softmax.\n",
    "\n",
    "Faster training for many classes – Because each model only sees a fraction of the data at a time.\n",
    "\n",
    "Works with all solvers – Supports liblinear, lbfgs, saga, etc.\n",
    "\n",
    "Limitations of OvR\n",
    "\n",
    "a. Can be slower during prediction – Since it evaluates multiple classifiers.\n",
    "\n",
    "b. May be less confident in classification – Since it only considers the highest probability and not all probabilities together.\n",
    "\n",
    "Softmax (Multinomial Logistic Regression)\n",
    "How It Works\n",
    "\n",
    "Trains a single model that directly predicts probabilities for all classes. Uses the Softmax function to normalize predictions so that all probabilities sum to 1.\n",
    "\n",
    "When to Choose Softmax Large datasets – Since it learns a single model, it performs well when there’s enough data.\n",
    "\n",
    "Better probability calibration – Outputs probabilities for all classes, making it useful for risk-based decisions.\n",
    "\n",
    "More natural extension of logistic regression – Works directly as a multiclass model instead of breaking the problem into multiple binary problems.\n",
    "\n",
    "Faster prediction time – Since only one model is used, it’s faster than OvR during inference.\n",
    "\n",
    "Limitations of Softmax\n",
    "\n",
    "Requires solvers that support multinomial classification – Only works well with lbfgs, newton-cg, or saga.\n",
    "\n",
    "Struggles with highly imbalanced classes – If some classes are much rarer than others, it may not learn well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72904699",
   "metadata": {},
   "source": [
    "20.How do we interpret coefficients in Logistic Regression\n",
    "\n",
    "Understanding the Logistic Regression Equation The logistic regression model is defined as:\n",
    "𝑃 ( 𝑦 = 1 ∣ 𝑋 ) = 1 1 + 𝑒 − ( 𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + . . . + 𝛽 𝑛 𝑋 𝑛 ) P(y=1∣X)= 1+e −(β 0​+β 1​X 1​+β 2​X 2​+...+β n​X n​)\n",
    "\n",
    "1​\n",
    "\n",
    "𝑃 ( 𝑦 = 1 ∣ 𝑋 ) P(y=1∣X) is the probability of the positive class. 𝛽 0 β 0​(Intercept) represents the baseline log-odds when all features are zero. 𝛽 1 , 𝛽 2 , . . . , 𝛽 𝑛 β 1​,β 2​,...,β n​are the coefficients for the corresponding features 𝑋 1 , 𝑋 2 , . . . , 𝑋 𝑛 X 1​,X 2​,...,X n​. The model predicts log-odds, not direct probabilities.\n",
    "\n",
    "Interpreting Coefficients in Terms of Log-Odds Each coefficient 𝛽 𝑖 β i​represents the change in log-odds when the corresponding feature increases by one unit, keeping all other features constant:\n",
    "log-odds = log ⁡ ( 𝑃 ( 𝑦 = 1 ) 1 − 𝑃 ( 𝑦 = 1 ) ) = 𝛽 0 + 𝛽 1 𝑋 1 + 𝛽 2 𝑋 2 + . . . + 𝛽 𝑛 𝑋 𝑛 log-odds=log( 1−P(y=1) P(y=1)​)=β 0​+β 1​X 1​+β 2​X 2​+...+β n​X n​\n",
    "\n",
    "If 𝛽 𝑖\n",
    "\n",
    "0 β i​\n",
    "\n",
    "0, increasing 𝑋 𝑖 X i​increases the probability of 𝑦 = 1 y=1. If 𝛽 𝑖 < 0 β i​<0, increasing 𝑋 𝑖 X i​decreases the probability of 𝑦 = 1 y=1. If 𝛽 𝑖 = 0 β i​=0, 𝑋 𝑖 X i​has no impact on the outcome.\n",
    "\n",
    "Converting Log-Odds to Odds Ratio To make the interpretation easier, we can exponentiate the coefficients to get the odds ratio:\n",
    "𝑒 𝛽 𝑖 e β i​\n",
    "\n",
    "Odds ratio > 1 → Feature increases the odds of 𝑦 = 1 y=1. Odds ratio < 1 → Feature decreases the odds of 𝑦 = 1 y=1. Odds ratio = 1 → Feature has no effect. Example: If 𝛽 1 = 0.7 β 1​=0.7 for feature 𝑋 1 X 1​, the odds ratio is:\n",
    "\n",
    "𝑒 0.7 ≈ 2.01 e 0.7 ≈2.01 This means increasing 𝑋 1 X 1​by one unit makes the positive outcome twice as likely.\n",
    "\n",
    "Interpreting Categorical Variables For binary categorical variables (0 or 1), the coefficient compares the effect of 1 vs. 0. For multi-category variables, one category is treated as the \"baseline,\" and other categories are compared to it. Example: If we have education levels (High School, Bachelor's, Master's), one category (e.g., High School) is set as the reference, and coefficients for Bachelor's and Master's compare against it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
