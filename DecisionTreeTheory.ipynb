{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc7c512",
   "metadata": {},
   "source": [
    "Q1.What is a Decision Tree, and how does it work?\n",
    "Ans.A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It is a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Decision Trees are popular because they are simple to understand, interpret, and visualize.\n",
    "\n",
    "A Decision Tree works by splitting data into smaller subsets based on feature values. It follows a hierarchical structure consisting of:\n",
    "Root Node: The starting point of the tree, representing the entire dataset.\n",
    "Decision Nodes: Internal nodes where the data is split based on a specific feature.\n",
    "Leaf Nodes: The final nodes that provide the output (class label in classification or numerical value in regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a139b815",
   "metadata": {},
   "source": [
    "Q2.What are impurity measures in Decision Trees?\n",
    "Ans.Impurity measures are used to evaluate how mixed (impure) a node is in a Decision Tree. The goal of a Decision Tree is to reduce impurity at each split, leading to pure nodes where all data points belong to the same class. The most common impurity measures are:\n",
    "\n",
    "1. Gini Impurity (Used in CART Algorithm)\n",
    "Formula:\n",
    "\n",
    "Gini=1− i=1∑n pi2\n",
    "where pi is the probability of a class in a node.\n",
    "\n",
    "2. Classification Error (Misclassification Rate)\n",
    "Formula:\n",
    "\n",
    "Error=1−max(pi)\n",
    "where pi is the probability of the majority class.\n",
    "Range: 0 (pure node) to 0.5 (equal class distribution).\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0227be98",
   "metadata": {},
   "source": [
    "Q3.What is the mathematical formula for Gini Impurity?\n",
    "Ans.Gini Impurity (Used in CART Algorithm)\n",
    "Formula:\n",
    "\n",
    "Gini=1− i=1∑n pi2\n",
    "where pi is the probability of a class in a node.\n",
    "\n",
    "The Gini Impurity measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly classified according to the distribution of labels in the set.\n",
    "A lower Gini Impurity means a purer node (closer to a single class).\n",
    "A higher Gini Impurity means a more mixed node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f176fa",
   "metadata": {},
   "source": [
    "Q4.What is the mathematical formula for Entropy?\n",
    "Ans.\n",
    "The mathematical formula for Entropy in a Decision Tree is:\n",
    "Entropy = − i=1∑npi log2 pi\n",
    "\n",
    "where:\n",
    "n is the number of classes.\n",
    "pi is the probability of a data point belonging to class i.\n",
    "The logarithm is base 2 (log2), which measures information in bits.\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41be44e",
   "metadata": {},
   "source": [
    "Q5.What is Information Gain, and how is it used in Decision Trees?\n",
    "Ans.Information Gain (IG) is a measure used in Decision Trees to determine the best feature to split on. It quantifies the reduction in entropy (uncertainty) after a dataset is split on a particular feature. The feature with the highest Information Gain is chosen for splitting.\n",
    "\n",
    "IG=Entropy(Parent) − j=1∑k Nj/N × Entropy(Childj)\n",
    "\n",
    "How Information Gain Works in Decision Trees\n",
    "1.Calculate the entropy of the parent node (before the split).\n",
    "2.Split the dataset based on a feature.\n",
    "3.Calculate entropy for each child node and compute the weighted sum of these entropies.\n",
    "4.Subtract the weighted entropy of the children from the parent entropy to get Information Gain.\n",
    "5.Choose the feature with the highest Information Gain for the split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9183a",
   "metadata": {},
   "source": [
    "6.What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "Formula: Gini Impurity:\n",
    "𝐺 𝑖 𝑛 𝑖 = 1 − ∑ 𝑖 = 1 𝑛 𝑝 𝑖 2 Gini=1− i=1 ∑ n​p i 2​\n",
    "\n",
    "Where 𝑝 𝑖 p i​is the probability of class 𝑖 i in the dataset. The Gini Impurity takes the sum of the squared probabilities of each class and subtracts it from 1.\n",
    "\n",
    "Entropy:\n",
    "\n",
    "𝐸 𝑛 𝑡 𝑟 𝑜 𝑝 𝑦 = − ∑ 𝑖 = 1 𝑛 𝑝 𝑖 log ⁡ 2 𝑝 𝑖 Entropy=− i=1 ∑ n​p i​log 2​p i​\n",
    "\n",
    "Where 𝑝 𝑖 p i​is the probability of class 𝑖 i in the dataset. Entropy measures the amount of uncertainty\n",
    "\n",
    "Range: Gini Impurity: Ranges from 0 to 1. A Gini Impurity of 0 means perfect purity (all elements belong to a single class), and a value of 1 means the dataset is equally distributed across all classes.\n",
    "Entropy: Ranges from 0 to log ⁡ 2 ( 𝑛 ) log 2​(n), where 𝑛 n is the number of classes in the dataset. A value of 0 means perfect purity, and higher values indicate more disorder.\n",
    "\n",
    "Interpretation of Values: Gini Impurity: Focuses more on the \"misclassification\" aspect. A lower Gini value means fewer misclassifications. Entropy: Measures the uncertainty or unpredictability in the dataset. Lower entropy means less uncertainty (more purity), and higher entropy means more uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aba538d",
   "metadata": {},
   "source": [
    "7.What is the mathematical explanation behind Decision Tree\n",
    "\n",
    "Mathematical Calculation for Information Gain (Entropy):\n",
    "Information Gain (IG) measures the reduction in uncertainty from a split. It is defined as the difference between the entropy of the original set and the weighted sum of the entropy of the subsets after the split.\n",
    "\n",
    "The formula for Information Gain is:\n",
    "\n",
    "𝐼 𝐺 ( 𝐷 , 𝐴 ) = 𝐸 𝑛 𝑡 𝑟 𝑜 𝑝 𝑦 ( 𝐷 ) − ∑ 𝑣 ∈ 𝑉 𝑎 𝑙 𝑢 𝑒 𝑠 ( 𝐴 ) ∣ 𝐷 𝑣 ∣ ∣ 𝐷 ∣ ⋅ 𝐸 𝑛 𝑡 𝑟 𝑜 𝑝 𝑦 ( 𝐷 𝑣 ) IG(D,A)=Entropy(D)− v∈Values(A) ∑​\n",
    "\n",
    "∣D∣ ∣D v​∣​⋅Entropy(D v​)\n",
    "\n",
    "Stopping Criteria:\n",
    "Max Depth: The tree can be limited by the maximum depth, meaning the number of splits it can make. Min Samples Split: A minimum number of data points required in a node to continue splitting. Pure Node: If the node reaches perfect purity (all data points belong to the same class), no further splitting occurs. Min Samples Leaf: A minimum number of samples required in a leaf node (final nodes of the tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac2190",
   "metadata": {},
   "source": [
    "8.What is Pre-Pruning in Decision Trees\n",
    "\n",
    "Pre-Pruning (also known as Early Stopping) in decision trees is a technique used to prevent overfitting by stopping the tree construction process before it becomes too complex. Essentially, it involves setting conditions that limit the growth of the tree during the training phase.\n",
    "\n",
    "Key Concepts of Pre-Pruning:\n",
    "\n",
    "Pre-pruning involves placing constraints on the tree as it is being built. Some of these constraints include:\n",
    "\n",
    "Maximum Depth:\n",
    "\n",
    "The maximum number of levels or layers the tree can have. Limiting the depth prevents the tree from growing too large and capturing unnecessary details that may not generalize well. For example, if the maximum depth is set to 3, the tree will only have 3 layers of splits, no matter how much the data could potentially benefit from further splits.\n",
    "\n",
    "Advantages of Pre-Pruning\n",
    "\n",
    "Reduces Overfitting by preventing the tree from capturing noise.\n",
    "\n",
    "Improves Generalization by ensuring the model works well on new data.\n",
    "\n",
    "Reduces Computation by limiting the number of splits, making the tree more efficient.\n",
    "\n",
    "Disadvantages of Pre-Pruning\n",
    "\n",
    "Risk of Underfitting if the stopping criteria are too strict.\n",
    "\n",
    "Difficult to Tune as selecting optimal thresholds requires experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a4361",
   "metadata": {},
   "source": [
    "9.What is Post-Pruning in Decision Trees\n",
    "\n",
    "Post-Pruning in Decision Trees\n",
    "\n",
    "Post-pruning (also called \"pruning after training\" or \"cost complexity pruning\") is a technique used to reduce the size of a fully grown decision tree by removing less significant branches. This helps improve generalization and prevents overfitting\n",
    "\n",
    "Advantages of Post-Pruning\n",
    "\n",
    " Reduces Overfitting – By eliminating unnecessary branches, the model generalizes better.\n",
    "\n",
    " Improves Interpretability – A smaller tree is easier to understand and visualize.\n",
    "\n",
    " Enhances Accuracy on New Data – Helps the model perform better on unseen examples.\n",
    "\n",
    "Disadvantages of Post-Pruning\n",
    "\n",
    " Requires Extra Computation – The pruning process adds an extra step after training.\n",
    "\n",
    " Risk of Removing Important Splits – If not done correctly, pruning may remove useful pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981ff05",
   "metadata": {},
   "source": [
    "10.What is the difference between Pre-Pruning and Post-Pruning\n",
    "\n",
    "Difference Between Pre-Pruning and Post-Pruning in Decision Trees\n",
    "\n",
    "Definition: Pre-pruning stops the tree from growing too large by applying constraints during training. In contrast, post-pruning allows the tree to grow fully first and then removes unnecessary branches after training.\n",
    "\n",
    "When Applied: Pre-pruning is done while the tree is being constructed, meaning it stops further splitting based on predefined conditions. Post-pruning is done after the tree has been fully built, by analyzing and trimming parts of the tree that do not contribute to better generalization.\n",
    "\n",
    "How it Works: Pre-pruning sets conditions such as maximum depth, minimum samples per node, or a threshold for information gain to decide whether a split should occur. If a split does not meet the criteria, it is prevented. Post-pruning first builds the tree completely and then removes less significant branches by evaluating their impact on accuracy using techniques like cost-complexity pruning or reduced-error pruning.\n",
    "\n",
    "Risk Factor: Pre-pruning risks underfitting if it stops the tree too early, leading to missed important patterns. Post-pruning, on the other hand, initially allows the tree to overfit before pruning it back to improve generalization.\n",
    "\n",
    "Computation Cost: Pre-pruning is computationally cheaper since it stops tree growth early. Post-pruning requires more computation because it involves growing the full tree first and then evaluating and trimming branches.\n",
    "\n",
    "Impact on Accuracy: Pre-pruning can sometimes limit model complexity too much, reducing its ability to learn from data effectively. Post-pruning usually provides a better balance between complexity and accuracy, as it first captures all possible patterns before deciding which ones to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a3343",
   "metadata": {},
   "source": [
    "Q11.What is a Decision Tree Regressor?\n",
    "Ans.A Decision Tree Regressor is a type of Decision Tree used for regression tasks. Unlike classification trees that predict discrete labels, a Decision Tree Regressor predicts continuous numerical values. It works by recursively splitting the data into smaller regions and fitting a constant value (mean or median) to each region.\n",
    "How Does a Decision Tree Regressor Work?\n",
    "1.Start with the entire dataset as the root node.\n",
    "2.Find the best feature to split the data, using a criterion like:\n",
    "*Mean Squared Error (MSE)\n",
    "*Mean Absolute Error (MAE)\n",
    "*Poisson Deviance\n",
    "3.Split the dataset at the chosen feature and threshold to minimize error.\n",
    "4.Repeat recursively until a stopping condition is met (e.g., max depth, minimum samples per leaf).\n",
    "5.Make predictions by assigning the mean or median value of data points in each leaf node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8858491",
   "metadata": {},
   "source": [
    "Q12.What are the advantages and disadvantages of Decision Trees\t?\n",
    "Ans.\n",
    "Advantages of Decision Trees\n",
    "1. Easy to Understand & Interpret\n",
    "Decision Trees mimic human decision-making, making them intuitive and easy to explain.\n",
    "Can be visualized in a tree structure.\n",
    "2. Handles Both Numerical & Categorical Data\n",
    "Works well with continuous (e.g., age, salary) and categorical (e.g., gender, yes/no) data.\n",
    "3. No Need for Feature Scaling\n",
    "Unlike SVMs or neural networks, Decision Trees don’t require normalization or standardization.\n",
    "4. Handles Missing Values\n",
    "Can handle missing data by using surrogate splits.\n",
    "\n",
    "Disadvantages of Decision Trees\n",
    "1. Prone to Overfitting\n",
    "Deep trees can memorize training data, leading to poor generalization on new data.\n",
    "Solution: Use pruning or ensemble methods (Random Forest, Gradient Boosting).\n",
    "2. Unstable (Sensitive to Small Changes in Data)\n",
    "A slight change in data can drastically change the tree structure.\n",
    "Solution: Use ensemble methods like Random Forest, which averages multiple trees.\n",
    "3. Biased with Imbalanced Data\n",
    "If certain classes dominate, the tree may favor them.\n",
    "Solution: Balance classes using resampling techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec9880",
   "metadata": {},
   "source": [
    "13.How does a Decision Tree handle missing values\n",
    "\n",
    "Ignoring Missing Values (Default in Some Implementations) Some decision tree algorithms, like CART (Classification and Regression Trees), can handle missing values internally. Instead of discarding data points with missing values, they use surrogate splits (alternative splits) based on other available features.\n",
    "Using Surrogate Splits If a feature used for splitting is missing in a data point, the tree finds a backup feature (surrogate) that gives a similar split. The model learns these surrogate splits during training by identifying features that closely mimic the original split. This method helps retain data instead of dropping missing values.\n",
    "Assigning the Most Frequent (or Mean/Median) Value For categorical features, missing values are often replaced with the most frequent category in that column. For numerical features, they can be replaced with the mean or median of that column. This is a simple but effective approach when missing values are minimal.\n",
    "Using Probabilistic Splitting Instead of assigning a single value, the model can distribute missing values proportionally based on the existing data distribution. If a feature is missing, the data point is assigned to different branches based on the probability of each possible value.\n",
    "Dropping Missing Data (Less Common) In some cases, when missing values are too frequent, dropping rows with missing values might be necessary. However, this can lead to significant data loss and is not recommended unless missing values are excessive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4271977",
   "metadata": {},
   "source": [
    "14.How does a Decision Tree handle categorical features\n",
    "\n",
    "One-Hot Encoding (for Nominal Categories)\n",
    "When dealing with categorical variables that have no inherent order (e.g., \"Red,\" \"Blue,\" \"Green\"), one common approach is one-hot encoding. This converts each category into a separate binary column (0 or 1).\n",
    "\n",
    "ex: Color: {Red, Blue, Green}\n",
    "→ Color_Red, Color_Blue, Color_Green\n",
    "\n",
    "2.Ordinal Encoding (for Ordered Categories)\n",
    "\n",
    "If the categories have a meaningful order (e.g., \"Low,\" \"Medium,\" \"High\"), we can assign numeric values: scss Copy Edit Low → 1, Medium → 2, High → 3\n",
    "The tree can then use these numerical values for splitting, similar to numerical features. However, if the order is not truly numeric, this can lead to misleading splits.\n",
    "\n",
    "Direct Splitting on Categorical Features\n",
    "Some decision tree implementations, such as CART (used in scikit-learn), do not natively support categorical splits. Instead, categorical features need to be encoded. However, other algorithms like C4.5 or CHAID can split directly on categorical values. The tree creates branches for each category, splitting the dataset accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27246f",
   "metadata": {},
   "source": [
    "Q15.What are some real-world applications of Decision Trees?\n",
    "Ans.\n",
    "1. Healthcare & Medical Diagnosis :\n",
    "Disease Diagnosis:\n",
    "Used to classify diseases based on symptoms (e.g., diagnosing diabetes, cancer, or heart disease).\n",
    "Example: A Decision Tree can predict whether a patient has COVID-19 based on fever, cough, and other symptoms.\n",
    "\n",
    "Medical Treatment Plans:\n",
    "Helps doctors decide which treatment to prescribe based on a patient’s medical history.\n",
    "2. Banking & Finance :\n",
    "Credit Risk Assessment:\n",
    "Banks use Decision Trees to determine whether to approve a loan based on income, credit score, and past defaults.\n",
    "\n",
    "Fraud Detection:\n",
    "Identifies fraudulent transactions by analyzing spending patterns.\n",
    "\n",
    "Stock Market Prediction:\n",
    "Helps predict stock price movements using historical data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
